{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14f406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "\n",
    "@contextmanager\n",
    "def connect_mongodb(username : str, password : str, host : str ,port : str):\n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "    client  = None\n",
    "    try:\n",
    "        print(\"Connecting to MongoDB\")\n",
    "        client = MongoClient(uri)\n",
    "        print(\"Connect to MongoDB\")\n",
    "        yield client\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "            print(\"Stop Connect to MongoDB\")\n",
    "        else:\n",
    "            print(\"Can't generate MongoDB Client\")\n",
    "        \n",
    "class MongoDB_Operation:\n",
    "    def __init__(self, client):\n",
    "        \"\"\"Initialize MongoDB connection.\"\"\"\n",
    "        if not isinstance(client, MongoClient):\n",
    "            raise TypeError(\"client must be an instance of MongoClient\")\n",
    "        self.client = client\n",
    "\n",
    "    def check_database_exists(self, db_name):\n",
    "        \"\"\"Check if a database exists.\"\"\"\n",
    "        return db_name in self.client.list_database_names()\n",
    "\n",
    "    def check_collection_exists(self, db_name, collection_name):\n",
    "        \"\"\"Check if a collection exists in a database.\"\"\"\n",
    "        return collection_name in self.client[db_name].list_collection_names()\n",
    "\n",
    "    def create_database(self, db_name):\n",
    "        \"\"\"Create a database if it does not exist.\"\"\"\n",
    "        if self.check_database_exists(db_name):\n",
    "            print(f\"Database '{db_name}' already exists!\")\n",
    "            return self.client[db_name]\n",
    "        else:\n",
    "            self.client[db_name].command(\"ping\")\n",
    "            print(f\"Database '{db_name}' has been created successfully.\")\n",
    "            return self.client[db_name]\n",
    "            \n",
    "    def create_collection(self, db_name, collection_name):\n",
    "        \"\"\"Create a collection if it does not exist.\"\"\"\n",
    "\n",
    "        db = self.create_database(db_name)\n",
    "\n",
    "        if self.check_collection_exists(db_name, collection_name):\n",
    "            print(f\"Collection '{collection_name}' already exists!\")\n",
    "            return db[collection_name]\n",
    "        else:\n",
    "            db.create_collection(collection_name)\n",
    "            print(f\"Collection '{collection_name}' has been created successfully.\")\n",
    "            return db[collection_name]\n",
    "\n",
    "    def find_data(self, db_name, collection_name, query=None):\n",
    "        \"\"\"Find data in a collection.\"\"\"\n",
    "        if not isinstance(query, dict):\n",
    "            raise TypeError(\"Query must be a dictionary\")\n",
    "        if not self.check_database_exists(db_name):\n",
    "            print(f\"Database '{db_name}' does not exist!\")\n",
    "        if not self.check_collection_exists(db_name, collection_name):\n",
    "            print(f\"Collection '{collection_name}' does not exist!\")\n",
    "\n",
    "        query = query or {}\n",
    "        data = self.client[db_name][collection_name].find(query)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def insert_many(self, db_name, collection_name, data : pd.DataFrame):\n",
    "        \"\"\"Insert multiple records into a collection.\"\"\"\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"Data must be a Pandas DataFrame\")\n",
    "        \n",
    "        coll = self.create_collection(db_name=db_name, collection_name=collection_name)\n",
    "\n",
    "        data_dict = data.to_dict(orient='records')\n",
    "\n",
    "        if data_dict:\n",
    "            coll.insert_many(data_dict)\n",
    "            print(\"Data has been added successfully.\")\n",
    "        else:\n",
    "            print(\"Empty data, nothing to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d1d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "API_KEY = \"2d6e1b290dabf74f65b84431677db2b8\"\n",
    "GET_URL = \"https://api.themoviedb.org/3/movie\"\n",
    "\n",
    "\n",
    "def get_movie_data(API_KEY, GET_URL, list_ids):\n",
    "    list_movies = []\n",
    "    for movie_id in list_ids:\n",
    "        url = f'{GET_URL}/{movie_id}?api_key={API_KEY}'\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        list_movies.append(data)\n",
    "    return list_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cebcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8ab49ce-52f9-4559-bd2c-55ee95ed8f30;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 2001ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8ab49ce-52f9-4559-bd2c-55ee95ed8f30\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/3ms)\n",
      "25/04/05 05:28:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/05 05:28:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bronze Process\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.1\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://ndtien_mongo:ndtien_mongo@mongodb:27017/\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://ndtien_mongo:ndtien_mongo@mongodb:27017/\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c15666-7698-4973-bc73-b697e1236408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.load.\n: com.mongodb.MongoTimeoutException: Timed out while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused}}]\n\tat com.mongodb.internal.connection.BaseCluster.createAndLogTimeoutException(BaseCluster.java:392)\n\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:148)\n\tat com.mongodb.internal.connection.SingleServerCluster.selectServer(SingleServerCluster.java:46)\n\tat com.mongodb.internal.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:108)\n\tat com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)\n\tat com.mongodb.client.internal.ClientSessionBinding.getReadConnectionSource(ClientSessionBinding.java:92)\n\tat com.mongodb.internal.operation.SyncOperationHelper.withSuppliedResource(SyncOperationHelper.java:141)\n\tat com.mongodb.internal.operation.SyncOperationHelper.withSourceAndConnection(SyncOperationHelper.java:122)\n\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$executeRetryableRead$4(SyncOperationHelper.java:186)\n\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$decorateReadWithRetries$12(SyncOperationHelper.java:289)\n\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:67)\n\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:191)\n\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:173)\n\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:189)\n\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:153)\n\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:153)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:130)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:116)\n\tat com.mongodb.client.internal.MongoIterableImpl.into(MongoIterableImpl.java:125)\n\tat com.mongodb.spark.sql.connector.schema.InferSchema.lambda$inferSchema$0(InferSchema.java:106)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withClient(AbstractMongoConfig.java:182)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withClient(ReadConfig.java:47)\n\tat com.mongodb.spark.sql.connector.schema.InferSchema.inferSchema(InferSchema.java:83)\n\tat com.mongodb.spark.sql.connector.MongoTableProvider.inferSchema(MongoTableProvider.java:60)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovies_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovies_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.load.\n: com.mongodb.MongoTimeoutException: Timed out while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused}}]\n\tat com.mongodb.internal.connection.BaseCluster.createAndLogTimeoutException(BaseCluster.java:392)\n\tat com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:148)\n\tat com.mongodb.internal.connection.SingleServerCluster.selectServer(SingleServerCluster.java:46)\n\tat com.mongodb.internal.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:108)\n\tat com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)\n\tat com.mongodb.client.internal.ClientSessionBinding.getReadConnectionSource(ClientSessionBinding.java:92)\n\tat com.mongodb.internal.operation.SyncOperationHelper.withSuppliedResource(SyncOperationHelper.java:141)\n\tat com.mongodb.internal.operation.SyncOperationHelper.withSourceAndConnection(SyncOperationHelper.java:122)\n\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$executeRetryableRead$4(SyncOperationHelper.java:186)\n\tat com.mongodb.internal.operation.SyncOperationHelper.lambda$decorateReadWithRetries$12(SyncOperationHelper.java:289)\n\tat com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:67)\n\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:191)\n\tat com.mongodb.internal.operation.SyncOperationHelper.executeRetryableRead(SyncOperationHelper.java:173)\n\tat com.mongodb.internal.operation.AggregateOperationImpl.execute(AggregateOperationImpl.java:189)\n\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:153)\n\tat com.mongodb.internal.operation.AggregateOperation.execute(AggregateOperation.java:44)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:153)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:130)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.forEach(MongoIterableImpl.java:116)\n\tat com.mongodb.client.internal.MongoIterableImpl.into(MongoIterableImpl.java:125)\n\tat com.mongodb.spark.sql.connector.schema.InferSchema.lambda$inferSchema$0(InferSchema.java:106)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withClient(AbstractMongoConfig.java:182)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withClient(ReadConfig.java:47)\n\tat com.mongodb.spark.sql.connector.schema.InferSchema.inferSchema(InferSchema.java:83)\n\tat com.mongodb.spark.sql.connector.MongoTableProvider.inferSchema(MongoTableProvider.java:60)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:140)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "            .format(\"mongodb\")\\\n",
    "            .option(\"database\", \"movies_db\")\\\n",
    "            .option(\"collection\", \"movies_name\")\\\n",
    "            .load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c248b1-890b-4d86-bb4b-391125a08ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "            .format(\"mongodb\")\\\n",
    "            .option(\"uri\",\"mongodb://ndtien_mongo:ndtien_mongo@mongodb:27017/movies_db.movies_name\")\\\n",
    "            .option(\"database\",\"movies_db\")\\\n",
    "            .option(\"collection\",\"movies_name\")\\\n",
    "            .load();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
